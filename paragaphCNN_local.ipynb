{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordEmbeddingDimension = 70\n",
    "vocabularySize=665000\n",
    "labels=5000\n",
    "filterSizes_paragraph = [3]\n",
    "filterSizes_allPara=3\n",
    "paragraphLength=100\n",
    "num_filters_parargaph=15\n",
    "num_filters_allPara=20\n",
    "maxParagraph = 7\n",
    "\n",
    "filterShapeOfAllPara =[filterSizes_allPara,3,1,num_filters_allPara]\n",
    "fullyConnectedLayerInput = 2100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullyConnectedLayerInput = 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2774\n",
      "966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class DataParser:\n",
    "    def __init__(self):\n",
    "        self.data=[]\n",
    "    \n",
    "    def getDataFromfile(self,fname):\n",
    "        self.counter =0\n",
    "        self.totalPages=0\n",
    "        f=open(fname)\n",
    "        self.data=[]\n",
    "        totalPages = int(f.readline())\n",
    "#         print(\"total Pages\",totalPages)\n",
    "        count=0\n",
    "        maxWordsInParagraph=paragraphLength\n",
    "        maxParagraphs=maxParagraph\n",
    "        totalLabels=labels\n",
    "\n",
    "        dummyParagraph =[0]*maxWordsInParagraph\n",
    "\n",
    "        # for pageId in f.readline():\n",
    "        while True:\n",
    "            pageId= f.readline()\n",
    "            if not pageId :\n",
    "                break\n",
    "\n",
    "            labelCount=int(f.readline())\n",
    "            labelsTemp=[0]*totalLabels\n",
    "            for i in range(labelCount):\n",
    "                tempLab=int(f.readline())\n",
    "                if tempLab<totalLabels:\n",
    "                    labelsTemp[tempLab]=1\n",
    "            instancesCount=int(f.readline())\n",
    "            instancesTemp=[]\n",
    "            for i in range(instancesCount):\n",
    "                tempInstance=f.readline().split()\n",
    "                temp=[int(x) for x in tempInstance]\n",
    "                for i in range(len(temp),maxWordsInParagraph):\n",
    "                    temp.append(0)\n",
    "                instancesTemp.append(temp[:maxWordsInParagraph])\n",
    "\n",
    "            for i in range(instancesCount,maxParagraphs):\n",
    "                instancesTemp.append(dummyParagraph)\n",
    "\n",
    "            self.data.append((labelsTemp,instancesTemp[:maxParagraphs]))\n",
    "#         self.data = data\n",
    "        self.totalPages = len(self.data)\n",
    "        f.close()\n",
    "        \n",
    "          \n",
    "        \n",
    "    def nextBatch(self):\n",
    "        if self.counter >=self.totalPages:\n",
    "            self.counter=0\n",
    "        data= self.data[self.counter]\n",
    "        self.counter+=1\n",
    "        return data\n",
    "    def restore(self):\n",
    "        self.counter=0\n",
    "training = DataParser()    \n",
    "training.getDataFromfile(\"data/training.txt\")\n",
    "\n",
    "# print(training.nextBatch())\n",
    "print(training.totalPages)\n",
    "training.restore()\n",
    "testing=  DataParser()\n",
    "testing.getDataFromfile(\"data/testing.txt\")\n",
    "print(testing.totalPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordEmbedding = tf.Variable(tf.random_uniform([vocabularySize, wordEmbeddingDimension], -1.0, 1.0),name=\"wordEmbedding\")\n",
    "\n",
    "paragraphList = []\n",
    "for i in range(maxParagraph):\n",
    "    paragraphList.append(tf.placeholder(tf.int32,[paragraphLength],name=\"paragraphPlaceholder\"+str(i)))\n",
    "\n",
    "target = tf.placeholder(tf.float32,[labels],name=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getParagraphEmbedding(paragraphWords):\n",
    "    device_name='cpu'\n",
    "    with tf.device(device_name): \n",
    "        paraEmbedding=tf.nn.embedding_lookup(wordEmbedding,paragraphWords)\n",
    "    \n",
    "    return tf.expand_dims(tf.expand_dims(paraEmbedding, -1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paragraphVector = getParagraphEmbedding(paragraphList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paragraphVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convLayeronParagraph(paragraphVector,filterSizes,num_input_channels,num_filters):\n",
    "    \n",
    "    pooled_outputs=[]\n",
    "    for filter_size in filterSizes:\n",
    "        shape = [filter_size,wordEmbeddingDimension,1,num_filters]\n",
    "        \n",
    "        weights = tf.Variable(tf.truncated_normal(shape, stddev=0.1),name=\"paragraphConvLayerW_\"+str(filter_size))\n",
    "        bias= tf.Variable(tf.constant(0.1, shape=[num_filters]),name=\"paragraphConvLayerB_\"+str(filter_size))\n",
    "        conv = tf.nn.conv2d(\n",
    "                    paragraphVector,\n",
    "                    weights,\n",
    "                    strides=[1, 1, wordEmbeddingDimension, 1],\n",
    "                    padding=\"SAME\",\n",
    "                    name=\"conv\")\n",
    "        \n",
    "        h = tf.nn.relu(tf.nn.bias_add(conv, bias), name=\"relu\")\n",
    "        pool_length=5\n",
    "        pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, pool_length, 1, 1],\n",
    "                    strides=[1, pool_length, 1, 1],\n",
    "                    padding='SAME',\n",
    "                    name=\"pool\")\n",
    "        pooled_outputs.append(pooled)\n",
    "    return tf.reshape(tf.concat(pooled_outputs,axis=0),[1,-1])\n",
    "#     filter_size = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paragraphCNNOutput = convLayeronParagraph (paragraphVector,filterSizes,1,num_filters_parargaph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paragraphCNNOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convLayerCombineParagraph(paragraphVectorList,filterSizes_paragraph,filterShapeOfAllPara,num_filters_parargaph,num_filters_allPara):\n",
    "    \n",
    "    paragraphCNNEmbedding=[]\n",
    "    \n",
    "    for paragraph in paragraphVectorList:\n",
    "        paragraphVector = getParagraphEmbedding(paragraph)\n",
    "        cnnEmbedding = convLayeronParagraph(paragraphVector,filterSizes_paragraph,1,num_filters_parargaph)\n",
    "        paragraphCNNEmbedding.append(cnnEmbedding)\n",
    "    \n",
    "    allParagraph=tf.expand_dims(tf.expand_dims(tf.concat(paragraphCNNEmbedding,axis=0),-1),0)\n",
    "    \n",
    "    shape = filterShapeOfAllPara\n",
    "    \n",
    "    weights= tf.Variable(tf.truncated_normal(shape, stddev=0.1),name=\"paragraphConvLayer2W_\"+str(filterShapeOfAllPara[0]))\n",
    "    bias= tf.Variable(tf.constant(0.1, shape=[num_filters_allPara]),name=\"paragraphConvLayer2B_\"+str(filterShapeOfAllPara[0]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(\n",
    "                    allParagraph,\n",
    "                    weights,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\",\n",
    "                    name=\"conv\")\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, bias), name=\"relu\")\n",
    "    return tf.reshape(allParagraph,[1,-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(convOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fullyConnectedLayer(convOutput,labels):\n",
    "    shape = [fullyConnectedLayerInput,labels]\n",
    "    weights =tf.Variable(tf.truncated_normal(shape, stddev=0.1),name=\"FC_W\")\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[labels]),name=\"FC_Bias\")\n",
    "    layer = tf.nn.sigmoid(tf.matmul(convOutput, weights) + bias)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device_name='cpu'\n",
    "with tf.device(device_name): \n",
    "    convOutput=convLayerCombineParagraph(paragraphList,filterSizes_paragraph,filterShapeOfAllPara,num_filters_parargaph,num_filters_allPara)\n",
    "    prediction=fullyConnectedLayer(convOutput,labels)\n",
    "    cross_entropy = -tf.reduce_sum(((target*tf.log(prediction + 1e-9)) + ((1-target) * tf.log(1 - prediction + 1e-9)) )  , name='xentropy' ) \n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "for e in range(epoch):\n",
    "    for itr in range(training.totalPages):\n",
    "        feed_dict_input={}\n",
    "        data= training.nextBatch()\n",
    "        feed_dict_input[target]=data[0]\n",
    "        for p in range(maxParagraph):\n",
    "            feed_dict_input[paragraphList[p]]= data[1][p]\n",
    "        cross_e=session.run(optimizer,feed_dict=feed_dict_input)\n",
    "#         cross_e=session.run(cross_entropy,feed_dict=feed_dict_input)\n",
    "#         print(cross_e)\n",
    "        if itr > 50:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(session, 'my_test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trueLabelsTest=[]\n",
    "predLabelsTest=[]\n",
    "for itr in range(testing.totalPages):\n",
    "    feed_dict_input={}\n",
    "    data= testing.nextBatch()\n",
    "    feed_dict_input[target]=data[0]\n",
    "    for p in range(maxParagraph):\n",
    "        feed_dict_input[paragraphList[p]]= data[1][p]\n",
    "    pred=session.run(prediction,feed_dict=feed_dict_input)\n",
    "    \n",
    "    trueLabelsTest.append(data[0])\n",
    "    predLabelsTest.append(pred[0])\n",
    "#     cross_e=session.run(cross_entropy,feed_dict=feed_dict_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=open(\"trueLabelesTest.txt\",\"w\")\n",
    "for i in trueLabelsTest:\n",
    "    w.write(str(i)+\"\\n\")\n",
    "w.close()\n",
    "\n",
    "w=open(\"predLabelsTest.txt\",\"w\")\n",
    "for j in predLabelsTest:\n",
    "    for i in j:\n",
    "        w.write(str(i)+\" \")\n",
    "    w.write(\"\\n\")\n",
    "w.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_e=session.run(cross_entropy,feed_dict=feed_dict_input)\n",
    "# print(cross_e)\n",
    "# pred=session.run(prediction,feed_dict=feed_dict_input)\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(pred)):\n",
    "#     if pred[0][i]>0.4:\n",
    "#         print(i)\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# session = tf.Session()\n",
    "# # session.run(tf.global_variables_initializer())\n",
    "# new_saver = tf.train.import_meta_graph('my_test_model.meta')\n",
    "# new_saver.restore(session, './my_test_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.run(tf.global_variables_initializer())\n",
    "# session.run(tf.all_variables())\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.run(prediction,feed_dict=feed_dict_input))\n",
    "session.run(cross_entropy,feed_dict=feed_dict_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vars = tf.trainable_variables()\n",
    "# for v in all_vars:\n",
    "#     print(v)\n",
    "#     print(session.run(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([v.op.name for v in tf.all_variables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.get_checkpoint_state('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
